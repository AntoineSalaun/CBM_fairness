{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a1df327-d2c4-4d92-bba3-7739fc96f7e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "377e03cb-92db-4a1b-ab33-9c3a3ba154c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import os \n",
    "sys.path.append('/home/gridsan/vyuan/.local/lib/python3.9/site-packages/')\n",
    "\n",
    "current_path = Path.cwd()\n",
    "\n",
    "# Go to top of the root and append\n",
    "root = current_path.parents[4]\n",
    "sys.path.append(str(root))\n",
    "\n",
    "os.chdir(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8ef0415-387d-475c-b8ef-ca594271b9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "import json\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debfc1b6-713e-4186-aa49-39956365f21e",
   "metadata": {},
   "source": [
    "## Code function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6dd64e4a-bff8-4b31-8a09-3497c7a11586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_original_data_and_metadata(verbs, path_imsitu, path_dataset):\n",
    "    original_metadata = json.load(open(path_imsitu / 'metadata' / 'full.json'))\n",
    "    path_original_data = path_imsitu / 'original_data'\n",
    "\n",
    "    selected_targets = {}\n",
    "\n",
    "    for verb in verbs:\n",
    "        selected_targets[verb] = original_metadata[verb]\n",
    "\n",
    "    target_concepts_count = {}\n",
    "    metadata = {}\n",
    "    metadata_by_gender = {}\n",
    "\n",
    "    list_male = ['male', 'man']\n",
    "    list_female = ['female', 'woman']\n",
    "    list_gender = ['male', 'man', 'female', 'woman']\n",
    "\n",
    "    # Loop through verbs\n",
    "    for verb in selected_targets:\n",
    "\n",
    "        target_concepts_count[verb] = {}\n",
    "        metadata[verb] = {}\n",
    "        metadata_by_gender[verb] = {'male': {}, 'female': {}}\n",
    "\n",
    "        path_dest = path_dataset / 'original' / verb\n",
    "        os.makedirs(path_dest / 'male')\n",
    "        os.makedirs(path_dest / 'female')\n",
    "\n",
    "        for image_id, original_metadata in selected_targets[verb].items():\n",
    "\n",
    "            # If agent is gendered, add the metadata and the concepts\n",
    "            if any(name in original_metadata['agent'] for name in list_gender):\n",
    "\n",
    "                image_agent = original_metadata['agent']\n",
    "                image_concepts = original_metadata['concepts']\n",
    "                metadata[verb][image_id] = original_metadata \n",
    "\n",
    "            for concept in image_concepts:\n",
    "                if concept not in target_concepts_count[verb]:\n",
    "                    target_concepts_count[verb][concept] = 1\n",
    "                else:\n",
    "                    target_concepts_count[verb][concept] += 1\n",
    "\n",
    "            # Depending on agent gender, copy metadata and file\n",
    "            if any(name in original_metadata['agent'] for name in list_female):\n",
    "                metadata_by_gender[verb]['female'][image_id] = original_metadata\n",
    "                shutil.copy(path_original_data / image_id, path_dest / 'female')\n",
    "\n",
    "            elif any(name in original_metadata['agent'] for name in list_male):\n",
    "                metadata_by_gender[verb]['male'][image_id] = original_metadata\n",
    "                shutil.copy(path_original_data / image_id, path_dest / 'male')\n",
    "\n",
    "\n",
    "    with open(path_dataset / 'metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "\n",
    "    with open(path_dataset / 'metadata_by_gender.json', 'w') as f:\n",
    "        json.dump(metadata_by_gender, f)\n",
    "\n",
    "    with open(path_dataset / 'target_concepts_count.json', 'w') as f: \n",
    "        json.dump(target_concepts_count, f)\n",
    "        \n",
    "def filter_concepts(path_dataset, threshold_target_concepts_retained):\n",
    "    \n",
    "    target_concepts_count = json.load(open(path_dataset / 'target_concepts_count.json'))\n",
    "\n",
    "    target_concepts_retained = {}\n",
    "\n",
    "    for verb in target_concepts_count:\n",
    "        target_concepts_retained[verb] = []\n",
    "        for concept in target_concepts_count[verb]:\n",
    "            if target_concepts_count[verb][concept] >= threshold_target_concepts_retained:\n",
    "                target_concepts_retained[verb].append(concept)\n",
    "\n",
    "    with open(path_dataset / 'target_concepts_retained.json', 'w') as f:\n",
    "        json.dump(target_concepts_retained, f)\n",
    "\n",
    "def balance_dataset(path_dataset):\n",
    "\n",
    "    metadata_with_gender = json.load(open(path_dataset / 'metadata_by_gender.json'))\n",
    "\n",
    "    path_dataset_original = path_dataset / 'original'\n",
    "    path_dataset_balanced = path_dataset / 'full_balanced'\n",
    "\n",
    "    metadata_full_balanced = {}\n",
    "\n",
    "    for target, value in metadata_with_gender.items():\n",
    "        min_samples = min(len(value['male']), len(value['female']))\n",
    "        metadata_full_balanced[target] = {'male': [], 'female': []}\n",
    "\n",
    "        # Note that sort will take '101' before '99'\n",
    "        for gender in ['male', 'female']:\n",
    "            path_src = path_dataset_original / target / gender\n",
    "            image_list = os.listdir(path_src)\n",
    "            image_list.sort()\n",
    "\n",
    "            path_dest = path_dataset_balanced / target / gender\n",
    "\n",
    "            if not os.path.exists(path_dest):\n",
    "                os.makedirs(path_dest)\n",
    "\n",
    "            for i in range(min_samples):\n",
    "                shutil.copy(path_src / image_list[i], path_dest / image_list[i])\n",
    "                metadata_full_balanced[target][gender].append(metadata_with_gender[target][gender][image_list[i]])\n",
    "\n",
    "    with open(path_dataset / 'metadata_full_balanced.json', 'w') as f: \n",
    "        json.dump(metadata_full_balanced, f)\n",
    "        \n",
    "def train_test_split_full_dataset(path_dataset, test_size=0.25, random_seed=0):\n",
    "    random.seed(random_seed)\n",
    "    \n",
    "    path_balanced = path_dataset / 'full_balanced'\n",
    "    targets = os.listdir(path_balanced)\n",
    "    \n",
    "    for target in targets:\n",
    "        path_target = path_balanced / target\n",
    "        \n",
    "        male_images = os.listdir(path_target / 'male')\n",
    "        female_images = os.listdir(path_target / 'female')\n",
    "        \n",
    "        male_train = random.sample(male_images, int(len(male_images) * (1 - test_size)))\n",
    "        female_train = random.sample(female_images, int(len(female_images) * (1 - test_size)))\n",
    "\n",
    "        male_test = list(set(male_images) - set(male_train))\n",
    "        female_test = list(set(female_images) - set(female_train))\n",
    "        \n",
    "        path_train = path_dataset / 'train' / 'train_full' / target\n",
    "        path_test = path_dataset / 'test' / target\n",
    "        path_test_with_gender = path_dataset / 'test_with_gender' / target\n",
    "\n",
    "        if not os.path.exists(path_train):\n",
    "            os.makedirs(path_train / 'male')\n",
    "            os.makedirs(path_train / 'female')\n",
    "            \n",
    "        if not os.path.exists(path_test_with_gender):\n",
    "            os.makedirs(path_test_with_gender / 'male')\n",
    "            os.makedirs(path_test_with_gender / 'female')\n",
    "            \n",
    "        if not os.path.exists(path_test):\n",
    "            os.makedirs(path_test)\n",
    "            \n",
    "        for image in male_train:\n",
    "            base, ext = os.path.splitext(image)\n",
    "            filename = f'{base}_male{ext}'\n",
    "            shutil.copy(path_target / 'male' / image, path_train / 'male' / filename)\n",
    "            \n",
    "        for image in female_train:\n",
    "            base, ext = os.path.splitext(image)\n",
    "            filename = f'{base}_female{ext}' \n",
    "            shutil.copy(path_target / 'female' / image, path_train / 'female' / filename)\n",
    "            \n",
    "        for image in male_test:\n",
    "            base, ext = os.path.splitext(image)\n",
    "            filename = f'{base}_male{ext}'\n",
    "            shutil.copy(path_target / 'male' / image, path_test / filename)\n",
    "            shutil.copy(path_target / 'male' / image, path_test_with_gender / 'male' / filename)\n",
    "            \n",
    "        for image in female_test:\n",
    "            base, ext = os.path.splitext(image)\n",
    "            filename = f'{base}_female{ext}' \n",
    "            shutil.copy(path_target / 'female' / image, path_test / filename) \n",
    "            shutil.copy(path_target / 'female' / image, path_test_with_gender / 'female' / filename) \n",
    "        \n",
    "def take_half_concepts(path_dataset):\n",
    "    metadata = json.load(open(path_dataset / 'metadata.json'))\n",
    "\n",
    "    list_target = sorted(metadata.keys())\n",
    "\n",
    "    cutoff = int(len(list_target) / 2)\n",
    "\n",
    "    first_half = list_target[:cutoff]\n",
    "    second_half = list_target[cutoff:]\n",
    "    \n",
    "    with open(path_dataset / 'verb_group_1.txt', 'w') as f:\n",
    "        for verb in first_half:\n",
    "            f.write(str(verb) + \"\\n\")\n",
    "        \n",
    "    with open(path_dataset / 'verb_group_2.txt', 'w') as f:\n",
    "        for verb in second_half:\n",
    "            f.write(str(verb) + \"\\n\")\n",
    "\n",
    "def sample_half_train(path_dataset, random_seed = 0):\n",
    "    random.seed(random_seed)\n",
    "    \n",
    "    targets = os.listdir(path_dataset / 'test')\n",
    "    path_train_full = path_dataset / 'train' / 'train_full'\n",
    "    path_train_half = path_dataset / 'train' / 'train_half'\n",
    "    \n",
    "    for target in targets:\n",
    "        os.makedirs(path_train_half / target)\n",
    "        \n",
    "        for gender in ['male', 'female']:\n",
    "            files = os.listdir(path_train_full / target / gender)\n",
    "            num_files = len(files)\n",
    "            train_half = random.sample(files, num_files // 2)\n",
    "            \n",
    "            for file in train_half:\n",
    "                shutil.copy(path_train_full / target / gender / file, path_train_half / target / file)\n",
    "                \n",
    "                \n",
    "def train_test_split_functioning_dataset(path_dataset):\n",
    "    path_test = path_dataset / 'test'\n",
    "    path_train_full = path_dataset / 'train' / 'train_full'\n",
    "    path_train_half = path_dataset / 'train' / 'train_half'\n",
    "    \n",
    "    path_train_test_split = path_dataset / 'train_test_split'\n",
    "    path_train_balanced = path_train_test_split / 'train_balanced'\n",
    "    path_train_imbalanced_1 = path_train_test_split / 'train_imbalanced_1'\n",
    "    path_train_imbalanced_2 = path_train_test_split / 'train_imbalanced_2'\n",
    "    \n",
    "    with open(path_dataset / 'verb_group_1.txt', 'r') as f:\n",
    "        verbs_group_1 = [line.strip() for line in f.readlines()]\n",
    "        \n",
    "    for target in os.listdir(path_test):\n",
    "        shutil.copytree(path_test / target, path_train_balanced / 'test' / target)\n",
    "        shutil.copytree(path_test / target, path_train_imbalanced_1 / 'test' / target)\n",
    "        shutil.copytree(path_test / target, path_train_imbalanced_2 / 'test' / target)\n",
    "        \n",
    "        shutil.copytree(path_train_half / target, path_train_balanced / 'train' / target)\n",
    "        \n",
    "        if target in verbs_group_1:\n",
    "            shutil.copytree(path_train_full / target / 'male', path_train_imbalanced_1 / 'train' / target)\n",
    "            shutil.copytree(path_train_full / target / 'female', path_train_imbalanced_2 / 'train' / target)\n",
    "        else:\n",
    "            shutil.copytree(path_train_full / target / 'female', path_train_imbalanced_1 / 'train' / target)\n",
    "            shutil.copytree(path_train_full / target / 'male', path_train_imbalanced_2 / 'train' / target)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5da3dd4-436b-4f56-9bd6-3e72da158ffa",
   "metadata": {},
   "source": [
    "## Call functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83216f13-1018-4b58-93d5-2bb01b2b270b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "string_verbs = \"\"\"\n",
    "phoning\n",
    "hugging\n",
    "eating\n",
    "admiring\n",
    "leaning\n",
    "putting\n",
    "carrying\n",
    "reading\n",
    "communicating\n",
    "hitchhiking\n",
    "resting\n",
    "vacuuming\n",
    "talking\n",
    "cleaning\n",
    "sitting\n",
    "patting\n",
    "hunching\n",
    "smelling\n",
    "rehabilitating\n",
    "perspiring\n",
    "shushing\n",
    "helping\n",
    "practicing\n",
    "pinning\n",
    "embracing\n",
    "drinking\n",
    "pushing\n",
    "crying\n",
    "rubbing\n",
    "feeding\n",
    "shouting\n",
    "tilting\n",
    "spying\n",
    "grinning\n",
    "picking\n",
    "caressing\n",
    "licking\n",
    "interviewing\n",
    "driving\n",
    "shivering\n",
    "milking\n",
    "grimacing\n",
    "shelving\n",
    "checking\n",
    "coughing\n",
    "manicuring\n",
    "lifting\n",
    "crouching\n",
    "kissing\n",
    "gasping\n",
    "instructing\n",
    "ignoring\n",
    "covering\n",
    "sleeping\n",
    "smiling\n",
    "measuring\n",
    "riding\n",
    "tasting\n",
    "stooping\n",
    "calling\n",
    "biting\n",
    "stroking\n",
    "jogging\n",
    "distributing\n",
    "washing\n",
    "giving\n",
    "asking\n",
    "stuffing\n",
    "tickling\n",
    "stripping\n",
    "heaving\n",
    "swinging\n",
    "winking\n",
    "browsing\n",
    "encouraging\n",
    "arranging\n",
    "hanging\n",
    "squeezing\n",
    "mopping\n",
    "photographing\n",
    "complaining\n",
    "baking\n",
    "brushing\n",
    "walking\n",
    "wiping\n",
    "standing\n",
    "chewing\n",
    "scrubbing\n",
    "scratching\n",
    "wheeling\n",
    "kneeling\n",
    "stretching\n",
    "snuggling\n",
    "shrugging\n",
    "telephoning\n",
    "staring\n",
    "working\n",
    "pinching\n",
    "buying\n",
    "dialing\n",
    "kicking\n",
    "sniffing\n",
    "opening\n",
    "speaking\n",
    "cooking\n",
    "waving\n",
    "studying\n",
    "slapping\n",
    "slouching\n",
    "frowning\n",
    "bothering\n",
    "praying\n",
    "adjusting\n",
    "buttoning\n",
    "sweeping\n",
    "applying\n",
    "yanking\n",
    "climbing\n",
    "signaling\n",
    "displaying\n",
    "pouting\n",
    "sneezing\n",
    "twirling\n",
    "recovering\n",
    "stirring\n",
    "scooping\n",
    "making\n",
    "whistling\n",
    "paying\n",
    "recuperating\n",
    "typing\n",
    "operating\n",
    "providing\n",
    "weeping\n",
    "shopping\n",
    "glaring\n",
    "emptying\n",
    "wrinkling\n",
    "exercising\n",
    "strapping\n",
    "crowning\n",
    "giggling\n",
    "running\n",
    "falling\n",
    "squinting\n",
    "gardening\n",
    "raking\n",
    "weighing\n",
    "smashing\n",
    "tripping\n",
    "serving\n",
    "yawning\n",
    "autographing\n",
    "rocking\n",
    "releasing\n",
    "immersing\n",
    "writing\n",
    "smearing\n",
    "shooting\n",
    "vaulting\n",
    "laughing\n",
    "crafting\n",
    "packaging\n",
    "counting\n",
    "placing\n",
    "tying\n",
    "cramming\n",
    "dragging\n",
    "saying\n",
    "pumping\n",
    "hoeing\n",
    "twisting\n",
    "buckling\n",
    "pouring\n",
    "painting\n",
    "lathering\n",
    "packing\n",
    "wringing\n",
    "combing\n",
    "taping\n",
    "tearing\n",
    "pasting\n",
    "frying\n",
    "confronting\n",
    "unpacking\n",
    "turning\n",
    "splashing\n",
    "saluting\n",
    "waiting\n",
    "offering\n",
    "misbehaving\n",
    "potting\n",
    "pedaling\n",
    "pressing\n",
    "jumping\n",
    "reassuring\n",
    "diving\n",
    "skating\n",
    "scolding\n",
    "begging\n",
    "\"\"\"\n",
    "list_verbs = verbs_list = string_verbs.strip().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7162137f-952e-4124-a016-6f38d05205a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_imsitu = Path('data/datasets/imSitu')\n",
    "\n",
    "path_original_data = path_imsitu / 'original_data'\n",
    "\n",
    "dataset_name = '200_verbs'\n",
    "path_dataset = Path('data/datasets/imSitu/data') / dataset_name\n",
    "\n",
    "threshold_target_concepts_retained = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1453ecb8-a78b-4b7b-acbc-4507f2b5b8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(list_verbs, path_imsitu, path_dataset, threshold_target_concepts_retained=15):\n",
    "    create_original_data_and_metadata(list_verbs, path_imsitu, path_dataset)\n",
    "    filter_concepts(path_dataset, threshold_target_concepts_retained)\n",
    "    balance_dataset(path_dataset)\n",
    "    train_test_split_full_dataset(path_dataset)\n",
    "    take_half_concept(path_dataset)\n",
    "    sample_half_train(path_dataset)\n",
    "    train_test_split_functioning_dataset(path_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdba0e2d-0528-4441-b210-a5e40dd0167a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = '200_verbs'\n",
    "path_imsitu = Path('data/datasets/imSitu')\n",
    "path_dataset = Path('data/datasets/imSitu/data') / dataset_name\n",
    "threshold_target_concepts_retained = 15\n",
    "\n",
    "pipeline(list_verbs, path_imsitu, path_dataset, threshold_target_concepts_retained)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a502217a-3046-4620-a548-1f8f9c9c8ae3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "109dfc05-df89-42d5-8264-7b1b6944de4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "take_half_concepts(path_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4afcb34b-9200-43b4-a994-86e37d6e2005",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs = list_verbs\n",
    "path_imsitu = Path('data/datasets/imSitu')\n",
    "full_targets = json.load(open(path_imsitu / 'metadata' / 'full.json'))\n",
    "\n",
    "selected_targets = {}\n",
    "\n",
    "for verb in verbs:\n",
    "    selected_targets[verb] = full_targets[verb]\n",
    "\n",
    "target_concepts_count = {}\n",
    "metadata = {}\n",
    "metadata_by_gender = {}\n",
    "\n",
    "list_male = ['male', 'man']\n",
    "list_female = ['female', 'woman']\n",
    "list_gender = ['male', 'man', 'female', 'woman']\n",
    "\n",
    "# Loop through verbs\n",
    "for verb in selected_targets:\n",
    "\n",
    "    target_concepts_count[verb] = {}\n",
    "    metadata[verb] = {}\n",
    "    metadata_by_gender[verb] = {'male': {}, 'female': {}}\n",
    "    \n",
    "    path_dest = path_dataset / 'original' / verb\n",
    "    os.makedirs(path_dest / 'male')\n",
    "    os.makedirs(path_dest / 'female')\n",
    "\n",
    "    for image_id, original_metadata in selected_targets[verb].items():\n",
    "        \n",
    "        # If agent is gendered, add the metadata and the concepts\n",
    "        if any(name in original_metadata['agent'] for name in list_gender):\n",
    "            \n",
    "            image_agent = original_metadata['agent']\n",
    "            image_concepts = original_metadata['concepts']\n",
    "            metadata[verb][image_id] = original_metadata \n",
    "            \n",
    "        for concept in image_concepts:\n",
    "            if concept not in target_concepts_count[verb]:\n",
    "                target_concepts_count[verb][concept] = 1\n",
    "            else:\n",
    "                target_concepts_count[verb][concept] += 1\n",
    "            \n",
    "        # Depending on agent gender, copy metadata and file\n",
    "        if any(name in original_metadata['agent'] for name in list_female):\n",
    "            metadata_by_gender[verb]['female'][image_id] = original_metadata\n",
    "            shutil.copy(path_original_data / image_id, path_dest / 'female')\n",
    "            \n",
    "        elif any(name in original_metadata['agent'] for name in list_male):\n",
    "            metadata_by_gender[verb]['male'][image_id] = original_metadata\n",
    "            shutil.copy(path_original_data / image_id, path_dest / 'male')\n",
    "\n",
    "            \n",
    "with open(path_dataset / 'metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f)\n",
    "    \n",
    "with open(path_dataset / 'metadata_by_gender.json', 'w') as f:\n",
    "    json.dump(metadata_by_gender, f)\n",
    "\n",
    "with open(path_dataset / 'target_concepts_count.json', 'w') as f: \n",
    "    json.dump(target_concepts_count, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381e8520-24ea-420b-af77-1b73d17e3cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_concepts_retained = {}\n",
    "\n",
    "for verb in target_concepts_count:\n",
    "    target_concepts_retained[verb] = []\n",
    "    for concept in target_concepts_count[verb]:\n",
    "        if target_concepts_count[verb][concept] >= threshold_target_concepts_retained:\n",
    "            target_concepts_retained[verb].append(concept)\n",
    "            \n",
    "with open(path_dataset / 'target_concepts_retained.json', 'w') as f:\n",
    "    json.dump(target_concepts_retained, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f811cdcf-0508-4cf7-9a9e-12fc0b8f2d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take half of the verbs\n",
    "# TODO: Write the verbs in a .txt file\n",
    "\n",
    "metadata = json.load(open(path_dataset / 'metadata.json'))\n",
    "\n",
    "list_target = sorted(metadata.keys())\n",
    "\n",
    "cutoff = int(len(list_target) / 2)\n",
    "\n",
    "first_half = list_target[:cutoff]\n",
    "second_half = list_target[cutoff:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2522882c-b78f-4d2f-8c49-0060273e8d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('verb_group_1.txt', 'w') as f:\n",
    "    for verb in first_half:\n",
    "        f.write(str(verb) + \"\\n\")\n",
    "        \n",
    "with open('verb_group_2.txt', 'w') as f:\n",
    "    for verb in second_half:\n",
    "        f.write(str(verb) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddf83fe6-27fa-41c7-ae63-8c96b804a8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_with_gender = json.load(open(path_dataset / 'metadata_by_gender.json'))\n",
    "\n",
    "path_dataset_original = path_dataset / 'original'\n",
    "path_dataset_balanced = path_dataset / 'full_balanced'\n",
    "\n",
    "metadata_full_balanced = {}\n",
    "\n",
    "for target, value in metadata_with_gender.items():\n",
    "    min_samples = min(len(value['male']), len(value['female']))\n",
    "    metadata_full_balanced[target] = {'male': [], 'female': []}\n",
    "    \n",
    "    # Note that sort will take '101' before '99'\n",
    "    for gender in ['male', 'female']:\n",
    "        path_src = path_dataset_original / target / gender\n",
    "        image_list = os.listdir(path_src)\n",
    "        image_list.sort()\n",
    "        \n",
    "        path_dest = path_dataset_balanced / target / gender\n",
    "        \n",
    "        if not os.path.exists(path_dest):\n",
    "            os.makedirs(path_dest)\n",
    "        \n",
    "        for i in range(min_samples):\n",
    "            shutil.copy(path_src / image_list[i], path_dest / image_list[i])\n",
    "            metadata_full_balanced[target][gender].append(metadata_with_gender[target][gender][image_list[i]])\n",
    "            \n",
    "with open(path_dataset / 'metadata_full_balanced.json', 'w') as f: \n",
    "    json.dump(metadata_full_balanced, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d35d756-aab9-4cea-af58-c1f641c6fb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(path_dataset, test_size=0.25, random_seed=0):\n",
    "    random.seed(random_seed)\n",
    "    \n",
    "    path_balanced = path_dataset / 'full_balanced'\n",
    "    targets = os.listdir(path_balanced)\n",
    "    \n",
    "    for target in targets:\n",
    "        path_target = path_balanced / target\n",
    "        \n",
    "        male_images = os.listdir(path_target / 'male')\n",
    "        female_images = os.listdir(path_target / 'female')\n",
    "        \n",
    "        male_train = random.sample(male_images, int(len(male_images) * (1 - test_size)))\n",
    "        female_train = random.sample(female_images, int(len(female_images) * (1 - test_size)))\n",
    "\n",
    "        male_test = list(set(male_images) - set(male_train))\n",
    "        female_test = list(set(female_images) - set(female_train))\n",
    "        \n",
    "        path_train = path_dataset / 'train' / 'train_full' / target\n",
    "        path_test = path_dataset / 'test' / target\n",
    "        path_test_with_gender = path_dataset / 'test_with_gender' / target\n",
    "\n",
    "        if not os.path.exists(path_train):\n",
    "            os.makedirs(path_train / 'male')\n",
    "            os.makedirs(path_train / 'female')\n",
    "            \n",
    "        if not os.path.exists(path_test_with_gender):\n",
    "            os.makedirs(path_test / 'male')\n",
    "            os.makedirs(path_test / 'female')\n",
    "            \n",
    "        if not os.path.exists(path_test):\n",
    "            os.makedirs(path_test)\n",
    "            \n",
    "        for image in male_train:\n",
    "            base, ext = os.path.splitext(image)\n",
    "            filename = f'{base}_male{ext}'\n",
    "            shutil.copy(path_target / 'male' / image, path_train / 'male' / filename)\n",
    "            \n",
    "        for image in female_train:\n",
    "            base, ext = os.path.splitext(image)\n",
    "            filename = f'{base}_female{ext}' \n",
    "            shutil.copy(path_target / 'female' / image, path_train / 'female' / filename)\n",
    "            \n",
    "        for image in male_test:\n",
    "            base, ext = os.path.splitext(image)\n",
    "            filename = f'{base}_male{ext}'\n",
    "            shutil.copy(path_target / 'male' / image, path_test / filename)\n",
    "            shutil.copy(path_target / 'male' / image, path_test_with_gender / 'male' / filename)\n",
    "            \n",
    "        for image in female_test:\n",
    "            base, ext = os.path.splitext(image)\n",
    "            filename = f'{base}_female{ext}' \n",
    "            shutil.copy(path_target / 'female' / image, path_test / filename) \n",
    "            shutil.copy(path_target / 'female' / image, path_test_with_gender / 'female' / filename) \n",
    "\n",
    "train_test_split(path_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1ce600ee-9523-4132-a595-3fed1ad1a0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_functioning_dataset(path_dataset):\n",
    "    path_test = path_dataset / 'test'\n",
    "    path_train_full = path_dataset / 'train' / 'train_full'\n",
    "    path_train_half = path_dataset / 'train' / 'train_half'\n",
    "    \n",
    "    path_train_test_split = path_dataset / 'train_test_split'\n",
    "    path_train_balanced = path_train_test_split / 'train_balanced'\n",
    "    path_train_imbalanced_1 = path_train_test_split / 'train_imbalanced_1'\n",
    "    path_train_imbalanced_2 = path_train_test_split / 'train_imbalanced_2'\n",
    "    \n",
    "    with open(path_dataset / 'verb_group_1.txt', 'r') as f:\n",
    "        verbs_group_1 = [line.strip() for line in f.readlines()]\n",
    "        \n",
    "    for target in os.listdir(path_test):\n",
    "        shutil.copytree(path_test / target, path_train_balanced / 'test' / target)\n",
    "        shutil.copytree(path_test / target, path_train_imbalanced_1 / 'test' / target)\n",
    "        shutil.copytree(path_test / target, path_train_imbalanced_2 / 'test' / target)\n",
    "        \n",
    "        shutil.copytree(path_train_half / target, path_train_balanced / 'train' / target)\n",
    "        \n",
    "        if target in verbs_group_1:\n",
    "            shutil.copytree(path_train_full / target / 'male', path_train_imbalanced_1 / 'train' / target)\n",
    "            shutil.copytree(path_train_full / target / 'female', path_train_imbalanced_2 / 'train' / target)\n",
    "        else:\n",
    "            shutil.copytree(path_train_full / target / 'female', path_train_imbalanced_1 / 'train' / target)\n",
    "            shutil.copytree(path_train_full / target / 'male', path_train_imbalanced_2 / 'train' / target)\n",
    "                        \n",
    "train_test_split_functioning_dataset(path_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce881bd-83e8-4136-9d3f-40059f4d5af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_balanced = 63 * 2\n",
    "test = 32 \n",
    "train_full = 47*2\n",
    "train_half = 46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d72165c4-20c9-4388-a434-b74c440c2828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_half_train(path_dataset, random_seed = 0):\n",
    "    random.seed(random_seed)\n",
    "    \n",
    "    targets = os.listdir(path_dataset / 'test')\n",
    "    path_train_full = path_dataset / 'train' / 'train_full'\n",
    "    path_train_half = path_dataset / 'train' / 'train_half'\n",
    "    \n",
    "    for target in targets:\n",
    "        os.makedirs(path_train_half / target)\n",
    "        \n",
    "        for gender in ['male', 'female']:\n",
    "            files = os.listdir(path_train_full / target / gender)\n",
    "            num_files = len(files)\n",
    "            train_half = random.sample(files, num_files // 2)\n",
    "            \n",
    "            for file in train_half:\n",
    "                shutil.copy(path_train_full / target / gender / file, path_train_half / target / file)\n",
    "                \n",
    "sample_half_train(path_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153616b3-534b-449f-8f49-15772a4584c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Works only for binary dataset\n",
    "def create_train_dataset(path_dataset, random_seed=0):\n",
    "\n",
    "    random.seed(random_seed)\n",
    "    categories = os.listdir(path_dataset / 'human_images' / 'datasets' / 'original_with_gender_balanced')\n",
    "\n",
    "    path_train_balanced = path_dataset / 'human_images' / 'datasets' / 'train_balanced'\n",
    "    path_train_imbalanced_1 = path_dataset / 'human_images' / 'datasets' / 'train_imbalanced_1'\n",
    "    path_train_imbalanced_2 =  path_dataset / 'human_images' / 'datasets' / 'train_imbalanced_2'\n",
    "\n",
    "    for i, category in enumerate(categories):\n",
    "        path_category = path_dataset / 'human_images' / 'datasets' / 'train' / category\n",
    "\n",
    "        for gender in ['male', 'female']:\n",
    "            path_category_gender = path_category / gender\n",
    "\n",
    "            if not os.path.exists(path_train_balanced / category):\n",
    "                os.makedirs(path_train_balanced / category)\n",
    "        \n",
    "            \n",
    "            files = os.listdir(path_category_gender)\n",
    "            num_files = len(files)\n",
    "            train_balanced = random.sample(files, int(num_files // 2)) \n",
    "\n",
    "            for file in train_balanced:\n",
    "                shutil.copy(path_category_gender / file, path_train_balanced / category / file)\n",
    "            \n",
    "            if i == 0:\n",
    "                if gender == 'male':\n",
    "                    shutil.copytree(path_category_gender, path_train_imbalanced_1 / category)\n",
    "                    with open(path_dataset / 'human_images' / 'class_male_1.txt', 'w') as f:\n",
    "                        f.write(f'{category}\\n')\n",
    "                else:\n",
    "                    shutil.copytree(path_category_gender, path_train_imbalanced_2 / category)\n",
    "            else:\n",
    "                if gender == 'female':\n",
    "                    shutil.copytree(path_category_gender, path_train_imbalanced_1 / category)\n",
    "                else:\n",
    "                    shutil.copytree(path_category_gender, path_train_imbalanced_2 / category)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8fedd7-be10-42f5-a6ee-31aa04a22b3e",
   "metadata": {},
   "source": [
    "# Code for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d121c718-d61f-4d99-b291-755d4608fb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "\n",
    "def create_dataset(verbs, path_dataset):\n",
    "\n",
    "    # Path to the directory containing the images\n",
    "    image_dir = 'data/datasets/imSitu/original_data'\n",
    "\n",
    "    # Path to the directory where the new folders will be created\n",
    "    output_dir = path_dataset / 'original'\n",
    "\n",
    "    # Loop through each verb\n",
    "    for verb in verbs:\n",
    "        # Create a new directory for the verb if it doesn't already exist\n",
    "        verb_dir = os.path.join(output_dir, verb)\n",
    "        if not os.path.exists(verb_dir):\n",
    "            os.makedirs(verb_dir)\n",
    "        \n",
    "        # Loop through each image in the image directory\n",
    "        for filename in os.listdir(image_dir):\n",
    "            # Check if the filename starts with the verb\n",
    "            if filename.startswith(verb):\n",
    "                # Copy the image to the verb's directory\n",
    "                src_path = os.path.join(image_dir, filename)\n",
    "                dst_path = os.path.join(verb_dir, filename)\n",
    "                shutil.copy(src_path, dst_path)\n",
    "\n",
    "def create_targets(verbs, path_dataset, threshold_target_concepts_retained=10):\n",
    "    \"\"\"\n",
    "    Creates the target_agent_count, target_concept_count and target_concepts_retained dictionaries\n",
    "    \"\"\"\n",
    "    path_imsitu = Path('data/datasets/imSitu')\n",
    "    full_targets = json.load(open(path_imsitu / 'metadata' / 'full.json'))\n",
    "\n",
    "    selected_targets = {}\n",
    "    for verb in verbs:\n",
    "        selected_targets[verb] = full_targets[verb]\n",
    "\n",
    "    target_concept_count = {}\n",
    "    target_agent_count = {}\n",
    "    for verb in selected_targets:\n",
    "        \n",
    "        target_concept_count[verb] = {}\n",
    "        target_agent_count[verb] = {}\n",
    "\n",
    "\n",
    "        for image in selected_targets[verb].values():\n",
    "            image_agent = image['agent']\n",
    "            image_concepts = image['concepts']\n",
    "            \n",
    "            if image_agent not in target_agent_count[verb]:\n",
    "                target_agent_count[verb][image_agent] = 1\n",
    "            else:\n",
    "                target_agent_count[verb][image_agent] += 1\n",
    "\n",
    "            for concept in image_concepts:\n",
    "                if concept not in target_concept_count[verb]:\n",
    "                    target_concept_count[verb][concept] = 1\n",
    "                else:\n",
    "                    target_concept_count[verb][concept] += 1\n",
    "            \n",
    "    target_concepts_retained = {}\n",
    "\n",
    "    for verb in target_concept_count:\n",
    "        target_concepts_retained[verb] = []\n",
    "        for concept in target_concept_count[verb]:\n",
    "            if target_concept_count[verb][concept] >= threshold_target_concepts_retained:\n",
    "                target_concepts_retained[verb].append(concept)\n",
    "\n",
    "    \"\"\"\n",
    "    print(target_agent_count)\n",
    "    print('_'*50)\n",
    "    print(target_concept_count)\n",
    "    print('_'*50)\n",
    "    print(target_concepts_retained)\n",
    "    print(type(target_agent_count))\n",
    "    print(type(target_concept_count))\n",
    "    print(type(target_concepts_retained))\n",
    "    \"\"\"\n",
    "\n",
    "    # Save target_agent_count dictionary to JSON\n",
    "    with open(path_dataset / 'target_agent_count.json', 'w') as f:\n",
    "        json.dump(target_agent_count, f)\n",
    "\n",
    "    # Save target_concept_count dictionary to JSON\n",
    "    with open(path_dataset / 'target_concept_count.json', 'w') as f:\n",
    "        json.dump(target_concept_count, f)\n",
    "\n",
    "    # Save target_concepts_retained dictionary to JSON\n",
    "    with open(path_dataset / 'target_concepts_retained.json', 'w') as f:\n",
    "        json.dump(target_concepts_retained, f)\n",
    "\n",
    "    with open(path_dataset / 'target_original_metadata.json', 'w') as f:\n",
    "        json.dump(selected_targets, f)\n",
    "\n",
    "def select_images_with_gender(path_dataset):\n",
    "\n",
    "    list_gender = ['man', 'male', 'woman', 'female']\n",
    "    target_original_metadata = json.load(open(path_dataset / 'target_original_metadata.json'))\n",
    "\n",
    "    entries_with_gender = {}\n",
    "\n",
    "    for target, images in target_original_metadata.items():\n",
    "        current_folder = path_dataset / 'original' / target\n",
    "        path_dest = path_dataset / 'human_images' / 'datasets' /'original' / target\n",
    "        entries_with_gender[target] = {}\n",
    "\n",
    "        if not os.path.exists(path_dest):\n",
    "            os.makedirs(path_dest)\n",
    "\n",
    "        for image_name, metadata in images.items():\n",
    "            if any(name in metadata['agent'] for name in list_gender):\n",
    "                shutil.copy(current_folder / image_name, path_dest / image_name)\n",
    "                entries_with_gender[target][image_name] = metadata\n",
    "    \n",
    "    with open(path_dataset / 'human_images' / 'metadata.json', 'w') as f:\n",
    "        json.dump(entries_with_gender, f)\n",
    "\n",
    "def select_images_with_gender_v2(path_dataset):\n",
    "    \"\"\"\n",
    "    Put images with their target and respective gender\n",
    "    \"\"\"\n",
    "    target_original_metadata = json.load(open(path_dataset / 'target_original_metadata.json'))\n",
    "    list_male = ['man', 'male']\n",
    "    list_female = ['woman', 'female']\n",
    "    entries_with_gender = {}\n",
    "\n",
    "    for target, images in target_original_metadata.items():\n",
    "        current_folder = path_dataset / 'original' / target\n",
    "        path_dest = path_dataset / 'human_images' / 'datasets' / 'original_with_gender' / target\n",
    "        entries_with_gender[target] = {'male': {}, 'female': {}}\n",
    "\n",
    "        if not os.path.exists(path_dest):\n",
    "            os.makedirs(path_dest / 'male')\n",
    "            os.makedirs(path_dest / 'female')\n",
    "\n",
    "        for image_name, metadata in images.items():\n",
    "            if any(name in metadata['agent'] for name in list_female):\n",
    "                entries_with_gender[target]['female'][image_name] = metadata\n",
    "                shutil.copy(current_folder / image_name, path_dest / 'female' / image_name)\n",
    "            elif any(name in metadata['agent'] for name in list_male):\n",
    "                entries_with_gender[target]['male'][image_name] = metadata\n",
    "                shutil.copy(current_folder / image_name, path_dest / 'male' / image_name)\n",
    "\n",
    "    # Metadata is same format as target original metadata: dict[target][image] = metadata\n",
    "    # Metadata V2 is dict[target][gender][image] = metadata\n",
    "    with open(path_dataset / 'human_images' / 'metadata_v2.json', 'w') as f:\n",
    "        json.dump(entries_with_gender, f)\n",
    "\n",
    "def train_test_split(path_dataset, test_size=0.25, random_seed=0):\n",
    "\n",
    "    random.seed(random_seed)\n",
    "    categories = os.listdir(path_dataset / 'human_images' / 'datasets' / 'original_with_gender_balanced')\n",
    "\n",
    "    for category in categories:\n",
    "        category_path = path_dataset / 'human_images' / 'datasets' / 'original_with_gender_balanced' / category\n",
    "\n",
    "        male_images = os.listdir(category_path / 'male')\n",
    "        female_images = os.listdir(category_path / 'female')\n",
    "\n",
    "        male_train = random.sample(male_images, int(len(male_images) * (1 - test_size)))\n",
    "        female_train = random.sample(female_images, int(len(female_images) * (1 - test_size)))\n",
    "\n",
    "        male_test = list(set(male_images) - set(male_train))\n",
    "        female_test = list(set(female_images) - set(female_train))\n",
    "\n",
    "        train_path = path_dataset / 'human_images' / 'datasets' / 'train' / category\n",
    "        test_path = path_dataset / 'human_images' / 'datasets' / 'test' / category\n",
    "        test_with_gender_path = path_dataset / 'human_images' / 'datasets' / 'test_with_gender' / category \n",
    "\n",
    "        if not os.path.exists(train_path):\n",
    "            os.makedirs(train_path / 'male')\n",
    "            os.makedirs(train_path / 'female')\n",
    "\n",
    "        if not os.path.exists(test_path):\n",
    "            os.makedirs(test_path)\n",
    "\n",
    "        if not os.path.exists(test_with_gender_path):\n",
    "            os.makedirs(test_with_gender_path / 'male')\n",
    "            os.makedirs(test_with_gender_path / 'female')\n",
    "        \n",
    "        for image in male_train:\n",
    "            base, ext = os.path.splitext(image)\n",
    "            filename = f'{base}_male{ext}'\n",
    "            shutil.copy(category_path / 'male' / image, train_path / 'male' / filename)\n",
    "        for image in female_train:\n",
    "            base, ext = os.path.splitext(image)\n",
    "            filename = f'{base}_female{ext}' \n",
    "            shutil.copy(category_path / 'female' / image, train_path / 'female' / filename)\n",
    "        for image in male_test:\n",
    "            base, ext = os.path.splitext(image)\n",
    "            filename = f'{base}_male{ext}'\n",
    "            shutil.copy(category_path / 'male' / image, test_path / filename)\n",
    "            shutil.copy(category_path / 'male' / image, test_with_gender_path / 'male' / filename)\n",
    "        for image in female_test:\n",
    "            base, ext = os.path.splitext(image)\n",
    "            filename = f'{base}_female{ext}' \n",
    "            shutil.copy(category_path / 'female' / image, test_path / filename) \n",
    "            shutil.copy(category_path / 'female' / image, test_with_gender_path / 'female' / filename) \n",
    "\n",
    "\n",
    "def create_copy_with_suffix(suffix):\n",
    "    def copy_with_suffix(src, dst):\n",
    "        base, ext = os.path.splitext(dst)\n",
    "        dst = f\"{base}_{suffix}{ext}\"\n",
    "        shutil.copy2(src, dst)\n",
    "    return copy_with_suffix\n",
    "\n",
    "\n",
    "# Works only for binary dataset\n",
    "def create_train_dataset(path_dataset, random_seed=0):\n",
    "\n",
    "    random.seed(random_seed)\n",
    "    categories = os.listdir(path_dataset / 'human_images' / 'datasets' / 'original_with_gender_balanced')\n",
    "\n",
    "    path_train_balanced = path_dataset / 'human_images' / 'datasets' / 'train_balanced'\n",
    "    path_train_imbalanced_1 = path_dataset / 'human_images' / 'datasets' / 'train_imbalanced_1'\n",
    "    path_train_imbalanced_2 =  path_dataset / 'human_images' / 'datasets' / 'train_imbalanced_2'\n",
    "\n",
    "    for i, category in enumerate(categories):\n",
    "        path_category = path_dataset / 'human_images' / 'datasets' / 'train' / category\n",
    "\n",
    "        for gender in ['male', 'female']:\n",
    "            path_category_gender = path_category / gender\n",
    "\n",
    "            if not os.path.exists(path_train_balanced / category):\n",
    "                os.makedirs(path_train_balanced / category)\n",
    "        \n",
    "            \n",
    "            files = os.listdir(path_category_gender)\n",
    "            num_files = len(files)\n",
    "            train_balanced = random.sample(files, int(num_files // 2)) \n",
    "\n",
    "            for file in train_balanced:\n",
    "                shutil.copy(path_category_gender / file, path_train_balanced / category / file)\n",
    "            \n",
    "            if i == 0:\n",
    "                if gender == 'male':\n",
    "                    shutil.copytree(path_category_gender, path_train_imbalanced_1 / category)\n",
    "                    with open(path_dataset / 'human_images' / 'class_male_1.txt', 'w') as f:\n",
    "                        f.write(f'{category}\\n')\n",
    "                else:\n",
    "                    shutil.copytree(path_category_gender, path_train_imbalanced_2 / category)\n",
    "            else:\n",
    "                if gender == 'female':\n",
    "                    shutil.copytree(path_category_gender, path_train_imbalanced_1 / category)\n",
    "                else:\n",
    "                    shutil.copytree(path_category_gender, path_train_imbalanced_2 / category)\n",
    "\n",
    "\n",
    "\n",
    "def create_balanced_dataset(path_dataset):\n",
    "\n",
    "    target_metadata = json.load(open(path_dataset / 'human_images' / 'metadata_v2.json'))\n",
    "\n",
    "    lowest_number = get_lowest_number(path_dataset)\n",
    "\n",
    "    path_dataset_balanced = path_dataset / 'human_images' / 'datasets' /'original_with_gender_balanced'\n",
    "\n",
    "    # Put images in the destination folder \n",
    "    # Take a balanced version, i.e. with the same number for every target and gender\n",
    "    # First destination has also gender, second destination has only targets\n",
    "    for target in target_metadata.keys():\n",
    "        for gender in ['male', 'female']:\n",
    "            \n",
    "            # The sort take '101' before '99' but it's not a problem\n",
    "            path_src = path_dataset / 'human_images' /'datasets' / 'original_with_gender' / target /  gender\n",
    "            image_list = os.listdir(path_src)\n",
    "            image_list.sort()\n",
    "            \n",
    "            path_dest = path_dataset_balanced / target / gender\n",
    "            \n",
    "            if not os.path.exists(path_dest):\n",
    "                os.makedirs(path_dest)\n",
    "            \n",
    "            for i in range(lowest_number):\n",
    "                shutil.copy(path_src / image_list[i] , path_dest / image_list[i])\n",
    "\n",
    "\n",
    "def get_lowest_number(path_dataset):\n",
    "    target_metadata = json.load(open(path_dataset / 'human_images' /'metadata_v2.json'))\n",
    "\n",
    "    lowest_number = 99999\n",
    "    for target, entries in target_metadata.items():\n",
    "        for gender, images in entries.items():\n",
    "            if len(images) < lowest_number:\n",
    "                lowest_number = len(images)\n",
    "    \n",
    "    return lowest_number\n",
    "\n",
    "\n",
    "def transfer_to_train_test_split(path_dataset):\n",
    "    path_test = path_dataset / 'human_images' / 'datasets' / 'test'\n",
    "    path_train_balanced = path_dataset / 'human_images' / 'datasets' / 'train_balanced'\n",
    "    path_train_imbalanced_1 = path_dataset / 'human_images' / 'datasets' / 'train_imbalanced_1'\n",
    "    path_train_imbalanced_2 = path_dataset / 'human_images' / 'datasets' / 'train_imbalanced_2'\n",
    "\n",
    "    path_test_with_gender_original = path_dataset / 'human_images' / 'datasets' / 'test_with_gender'\n",
    "    path_test_with_gender_new = path_dataset / 'human_images' / 'test'\n",
    "\n",
    "    path_train_test_split = path_dataset / 'human_images' / 'train_test_split'\n",
    "\n",
    "    for category in os.listdir(path_test):\n",
    "        shutil.copytree(path_test / category, path_train_test_split / 'balanced' / 'test' / category)\n",
    "        shutil.copytree(path_test / category, path_train_test_split / 'imbalanced_1' / 'test' / category)\n",
    "        shutil.copytree(path_test / category, path_train_test_split / 'imbalanced_2' / 'test' / category)\n",
    "\n",
    "        shutil.copytree(path_train_balanced / category, path_train_test_split / 'balanced' / 'train' / category)\n",
    "        shutil.copytree(path_train_imbalanced_1 / category, path_train_test_split / 'imbalanced_1' / 'train' / category)\n",
    "        shutil.copytree(path_train_imbalanced_2 / category, path_train_test_split / 'imbalanced_2' / 'train' / category)\n",
    "        for gender in ['male', 'female']:\n",
    "           shutil.copytree(path_test_with_gender_original / category / gender, path_test_with_gender_new / f'{category}_{gender}') \n",
    "\n",
    "\n",
    "def pipeline(verbs, dataset_name):\n",
    "    path_dataset = Path('data/datasets/imSitu/data') / dataset_name\n",
    "    create_dataset(verbs, path_dataset)\n",
    "    create_targets(verbs, path_dataset, 10)\n",
    "    select_images_with_gender(path_dataset)\n",
    "    select_images_with_gender_v2(path_dataset)\n",
    "    create_balanced_dataset(path_dataset)\n",
    "    train_test_split(path_dataset)\n",
    "    create_train_dataset(path_dataset)\n",
    "    transfer_to_train_test_split(path_dataset)\n",
    "    # create_imbalanced_datasets_binary(path_dataset)\n",
    "    \n",
    "\n",
    "\n",
    "phoning_eating = ['phoning', 'eating']\n",
    "dataset_name = 'phoning_eating'\n",
    "pipeline(phoning_eating, dataset_name)\n",
    "\n",
    "#path_dataset = Path('data/datasets/imSitu/data/phoning_cooking')\n",
    "\n",
    "\"\"\"\n",
    "verbs = ['cooking', 'driving', 'cleaning', 'phoning']\n",
    "phoning_eating = ['phoning', 'eating']\n",
    "create_dataset(phoning_eating)\n",
    "create_targets(phoning_eating, 10)\n",
    "\"\"\"\n",
    "# select_images_with_gender_v2(path_dataset)\n",
    "# create_balanced_dataset(Path('data/datasets/imSitu/data/phoning_cooking/human_images'))\n",
    "# create_imbalanced_datasets_binary(Path('data/datasets/imSitu/data/phoning_cooking/human_images'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
